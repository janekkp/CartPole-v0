{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPole-v0 Q-agent",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janekkp/CartPole-v0/blob/colab/CartPole_v0_Q_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB4rqr5El1GM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "from collections import defaultdict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onpQJVbtmJQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Qlearn_agent():\n",
        "  def __init__(self, env = gym.make('CartPole-v0'), alpha = 0.15, num_of_episodes = 1000, gamma = 1, epsilon = 1):\n",
        "    self.env = env\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_decay = 0.999\n",
        "    self.num_of_episodes = num_of_episodes\n",
        "    self.scores = []\n",
        "\n",
        "    self.num_of_actions = self.env.action_space.n\n",
        "    self.Q = defaultdict(lambda: np.zeros(self.num_of_actions))\n",
        "    \n",
        "    self.range_cart_position = np.linspace(-2.4, 2.4, num = 1)\n",
        "    self.range_cart_velocity = np.linspace(-10, 10, num = 1)\n",
        "    self.range_pole_angle = np.linspace(-15, 15, num = 800)\n",
        "    self.range_pole_velocity = np.linspace(-20, 20, num = 90)\n",
        "    \n",
        "  def discretize(self, state):\n",
        "    #Converting continous space into discrete\n",
        "    cart_position, cart_velocity, pole_angle, pole_velocity = state\n",
        "    \n",
        "    cart_position = np.digitize(cart_position, self.range_cart_position)\n",
        "    cart_velocity = np.digitize(cart_velocity, self.range_cart_velocity)\n",
        "    pole_angle = np.digitize(pole_angle, self.range_pole_angle)\n",
        "    pole_velocity = np.digitize(pole_velocity, self.range_pole_velocity)\n",
        "        \n",
        "    state = cart_position, cart_velocity, pole_angle, pole_velocity\n",
        "    \n",
        "    return state\n",
        "  \n",
        "\n",
        "  def train(self, num_of_episodes):\n",
        "    #Training stage, where agent picks actions randomly. \n",
        "    #Update of q-values is done with standard Q-learing algorithm\n",
        "    for episode in range(self.num_of_episodes):      \n",
        "      s = self.discretize(self.env.reset())\n",
        "      self.epsilon = self.epsilon*self.epsilon_decay\n",
        "      while True:\n",
        "        action = self.env.action_space.sample() if (random.random() <= self.epsilon) else np.argmax(self.Q[s])\n",
        "        next_state, reward, end, _ = self.env.step(action)\n",
        "        next_state = self.discretize(next_state)\n",
        "        if end:\n",
        "          self.Q[s][action] = reward\n",
        "          break\n",
        "        else:\n",
        "          self.Q[s][action] += self.alpha * (reward + self.gamma * np.max(self.Q[next_state]) - self.Q[s][action])\n",
        "          s = next_state\n",
        "        \n",
        "    return self.Q\n",
        "  \n",
        "  def play(self, state):\n",
        "    #At this stage agent picks learned actions\n",
        "    s = self.discretize(state)\n",
        "    return np.argmax(self.Q[s])\n",
        "  \n",
        "  def ewaluate_playing(self):\n",
        "    #CartPole is solved if mean score over 100 consecutive trials is >= 195\n",
        "    for i in range(100):\n",
        "      score = 0\n",
        "      s = self.discretize(self.env.reset())\n",
        "      end = False\n",
        "      while not end:\n",
        "        a = self.play(s)\n",
        "        s, r, end, _ = self.env.step(a)\n",
        "        score += r\n",
        "        if end:\n",
        "          self.scores.append(score)\n",
        "          break\n",
        "      return np.mean(self.scores)\n",
        "    \n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2vFL-Vd_hB0",
        "colab_type": "code",
        "outputId": "872da765-7f28-4179-f65a-6d9ca475c7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def main():\n",
        "  player = Qlearn_agent()\n",
        "  player.train(player.num_of_episodes)\n",
        "  print('Qlearn_agent obtained {} score over 100 consecutive trials, after {} episodes of training'.format(player.ewaluate_playing(), player.num_of_episodes))\n",
        "  \n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "    "
      ],
      "execution_count": 379,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Qlearn_agent obtained 200.0 score over 100 consecutive trials, after 1000 episodes of training\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}